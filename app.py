# -*- coding: utf-8 -*-
import numpy as np
import streamlit as st
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM


st.set_page_config(
    page_title="ì¿¼ì¹´ê³ ", layout="wide", initial_sidebar_state="expanded"
)

@st.cache
def load_model(model_name):
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    return model

tokenizer = AutoTokenizer.from_pretrained("QuoQA-NLP/KE-T5-Ko2En-Base")
ko2en_model = load_model("QuoQA-NLP/KE-T5-Ko2En-Base")
en2ko_model = load_model("QuoQA-NLP/KE-T5-En2Ko-Base")


st.title("ðŸ» ì¿¼ì¹´ê³  ë²ˆì—­ê¸°")
st.write("ì¢Œì¸¡ì— ë²ˆì—­ ëª¨ë“œë¥¼ ì„ íƒí•˜ê³ , CTRL+Enter(CMD+Enter)ë¥¼ ëˆ„ë¥´ì„¸ìš” ðŸ¤—")
st.write("Select Translation Mode at the left and press CTRL+Enter(CMD+Enter)ðŸ¤—")

translation_list = ["í•œêµ­ì–´ì—ì„œ ì˜ì–´ | Korean to English", "ì˜ì–´ì—ì„œ í•œêµ­ì–´ | English to Korean"]
translation_mode = st.sidebar.radio("ë²ˆì—­ ëª¨ë“œë¥¼ ì„ íƒ(Translation Mode):", translation_list)


default_value = 'ì‹ í•œì¹´ë“œ ê´€ê³„ìžëŠ” "ê³¼ê±° ë‚´ë†“ì€ ìƒí’ˆì˜ ê²½ìš° ì¶œì‹œ 2ê°œì›” ë§Œì— ì ê¸ˆ ê°€ìž…ì´ 4ë§Œì—¬ ì¢Œì— ë‹¬í•  ì •ë„ë¡œ ì¸ê¸°ë¥¼ ëŒì—ˆë‹¤"ë©´ì„œ "ê¸ˆë¦¬ ì¸ìƒì— ë”°ë¼ ì ê¸ˆ ê¸ˆë¦¬ë¥¼ ë” ì˜¬ë ¤ ë§Žì€ ê³ ê°ì´ ëª°ë¦´ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•˜ê³  ìžˆë‹¤"ê³  ë§í–ˆë‹¤.'
src_text = st.text_area(
    "ë²ˆì—­í•˜ê³  ì‹¶ì€ ë¬¸ìž¥ì„ ìž…ë ¥í•˜ì„¸ìš”:",
    default_value,
    height=300,
    max_chars=200,
)
print(src_text)



if src_text == "":
    st.warning("Please **enter text** for translation")
else: 
    # translate into english sentence
    if translation_mode == translation_list[0]:
        model = ko2en_model
    else: 
        model = en2ko_model

    translation_result = model.generate(
        **tokenizer(
            src_text,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=64,
        ),
        max_length=64,
        num_beams=5,
        repetition_penalty=1.3,
        no_repeat_ngram_size=3,
        num_return_sequences=1,
    )
    translation_result = tokenizer.decode(
        translation_result[0],
        clean_up_tokenization_spaces=True,
        skip_special_tokens=True,
    )

    print(f"{src_text} -> {translation_result}")

    st.write(translation_result)
    print(translation_result)
